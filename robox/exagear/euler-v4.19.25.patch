diff --git a/arch/arm64/include/asm/compat.h b/arch/arm64/include/asm/compat.h
--- a/arch/arm64/include/asm/compat.h	2019-04-29 10:13:10.606997832 +0800
+++ b/arch/arm64/include/asm/compat.h	2019-04-29 10:13:32.166998400 +0800
@@ -225,6 +225,15 @@
 
 static inline int is_compat_task(void)
 {
+#ifdef CONFIG_TANGO_BT
+	return current_thread_info()->tango_syscall || test_thread_flag(TIF_32BIT);
+#else
+	return test_thread_flag(TIF_32BIT);
+#endif
+}
+
+static inline int is_aarch32_compat_task(void)
+{
 	return test_thread_flag(TIF_32BIT);
 }
 
@@ -233,6 +242,13 @@
 	return test_ti_thread_flag(thread, TIF_32BIT);
 }
 
+#ifdef CONFIG_TANGO_BT
+static inline int is_tango_compat_task(void)
+{
+	return current_thread_info()->tango_syscall;
+}
+#endif
+
 #else /* !CONFIG_COMPAT */
 
 static inline int is_compat_thread(struct thread_info *thread)
diff --git a/arch/arm64/include/asm/mmu.h b/arch/arm64/include/asm/mmu.h
--- a/arch/arm64/include/asm/mmu.h	2019-04-29 10:13:10.606997832 +0800
+++ b/arch/arm64/include/asm/mmu.h	2019-04-29 10:13:32.166998400 +0800
@@ -29,6 +29,9 @@
 	atomic64_t	id;
 	void		*vdso;
 	unsigned long	flags;
+#ifdef CONFIG_TANGO_BT
+	unsigned long	tango_mmap_base;
+#endif
 } mm_context_t;
 
 #define MAX_RES_REGIONS	32
diff --git a/arch/arm64/include/asm/pgtable.h b/arch/arm64/include/asm/pgtable.h
--- a/arch/arm64/include/asm/pgtable.h	2019-04-29 10:13:10.606997832 +0800
+++ b/arch/arm64/include/asm/pgtable.h	2019-04-29 10:13:32.166998400 +0800
@@ -780,6 +780,16 @@
 #define phys_to_ttbr(addr)	(addr)
 #endif
 
+/*
+ * We provide our own arch_get_unmapped_area to handle 32-bit mmap calls from
+ * tango.
+ */
+#ifdef CONFIG_TANGO_BT
+#define HAVE_ARCH_UNMAPPED_AREA
+#define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
+#define HAVE_ARCH_HUGETLB_UNMAPPED_AREA
+#endif
+
 #endif /* !__ASSEMBLY__ */
 
 #endif /* __ASM_PGTABLE_H */
diff --git a/arch/arm64/include/asm/processor.h b/arch/arm64/include/asm/processor.h
--- a/arch/arm64/include/asm/processor.h	2019-04-29 10:13:10.606997832 +0800
+++ b/arch/arm64/include/asm/processor.h	2019-04-29 10:13:32.166998400 +0800
@@ -217,8 +217,31 @@
 #define task_pt_regs(p) \
 	((struct pt_regs *)(THREAD_SIZE + task_stack_page(p)) - 1)
 
+#ifdef CONFIG_TANGO_BT
+
+#define KSTK_EIP(tsk)							\
+({									\
+	unsigned long __out;						\
+	if (task_thread_info(tsk)->tango_syscall)			\
+		__out = (unsigned long)task_pt_regs(tsk)->regs[15];	\
+	else								\
+		__out = (unsigned long)task_pt_regs(tsk)->pc;		\
+	__out;								\
+ })
+#define KSTK_ESP(tsk)							\
+({									\
+	unsigned long __out;						\
+	if (task_thread_info(tsk)->tango_syscall)			\
+		__out = (unsigned long)task_pt_regs(tsk)->regs[13];	\
+	else								\
+		__out = user_stack_pointer(task_pt_regs(tsk));		\
+	__out;								\
+ })
+
+#else
 #define KSTK_EIP(tsk)	((unsigned long)task_pt_regs(tsk)->pc)
 #define KSTK_ESP(tsk)	user_stack_pointer(task_pt_regs(tsk))
+#endif
 
 /*
  * Prefetching support
diff --git a/arch/arm64/include/asm/thread_info.h b/arch/arm64/include/asm/thread_info.h
--- a/arch/arm64/include/asm/thread_info.h	2019-04-29 10:13:10.606997832 +0800
+++ b/arch/arm64/include/asm/thread_info.h	2019-04-29 10:13:32.166998400 +0800
@@ -43,6 +43,9 @@
 	u64			ttbr0;		/* saved TTBR0_EL1 */
 #endif
 	int			preempt_count;	/* 0 => preemptable, <0 => bug */
+#ifdef CONFIG_TANGO_BT
+	int			tango_syscall;	/* tango 32-bit syscall */
+#endif
 };
 
 #define thread_saved_pc(tsk)	\
diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig
--- a/arch/arm64/Kconfig	2019-04-29 10:13:10.586997832 +0800
+++ b/arch/arm64/Kconfig	2019-04-29 10:13:32.166998400 +0800
@@ -956,6 +956,16 @@
 	help
 	  Say Y if you want to run Linux in a Virtual Machine on Xen on ARM64.
 
+config TANGO_BT
+	bool "Tango binary translator support"
+	depends on COMPAT
+	select CHECKPOINT_RESTORE
+	select BINFMT_MISC
+	default y
+	help
+	  Kernel support for the Tango binary translator which dynamically
+	  translates 32-bit ARM binaries into 64-bit code.
+
 config FORCE_MAX_ZONEORDER
 	int
 	default "14" if (ARM64_64K_PAGES && TRANSPARENT_HUGEPAGE)
diff --git a/arch/arm64/kernel/asm-offsets.c b/arch/arm64/kernel/asm-offsets.c
--- a/arch/arm64/kernel/asm-offsets.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/asm-offsets.c	2019-04-29 10:13:32.162998400 +0800
@@ -46,6 +46,9 @@
   DEFINE(TSK_TI_TTBR0,		offsetof(struct task_struct, thread_info.ttbr0));
 #endif
   DEFINE(TSK_STACK,		offsetof(struct task_struct, stack));
+#ifdef CONFIG_TANGO_BT
+  DEFINE(TI_TANGO_SYSCALL,	offsetof(struct task_struct, thread_info.tango_syscall));
+#endif
   BLANK();
   DEFINE(THREAD_CPU_CONTEXT,	offsetof(struct task_struct, thread.cpu_context));
   BLANK();
diff --git a/arch/arm64/kernel/fpsimd.c b/arch/arm64/kernel/fpsimd.c
--- a/arch/arm64/kernel/fpsimd.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/fpsimd.c	2019-04-29 10:13:32.162998400 +0800
@@ -806,7 +806,7 @@
 asmlinkage void do_sve_acc(unsigned int esr, struct pt_regs *regs)
 {
 	/* Even if we chose not to use SVE, the hardware could still trap: */
-	if (unlikely(!system_supports_sve()) || WARN_ON(is_compat_task())) {
+	if (unlikely(!system_supports_sve()) || WARN_ON(is_aarch32_compat_task())) {
 		force_signal_inject(SIGILL, ILL_ILLOPC, regs->pc);
 		return;
 	}
diff --git a/arch/arm64/kernel/process.c b/arch/arm64/kernel/process.c
--- a/arch/arm64/kernel/process.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/process.c	2019-04-29 10:13:32.162998400 +0800
@@ -323,7 +323,7 @@
 {
 	write_sysreg(0, tpidr_el0);
 
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		current->thread.uw.tp_value = 0;
 
 		/*
@@ -552,7 +552,7 @@
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	if (is_compat_task())
+	if (is_aarch32_compat_task())
 		return randomize_page(mm->brk, SZ_32M);
 	else
 		return randomize_page(mm->brk, SZ_1G);
@@ -563,7 +563,10 @@
  */
 void arch_setup_new_exec(void)
 {
-	current->mm->context.flags = is_compat_task() ? MMCF_AARCH32 : 0;
+#ifdef CONFIG_TANGO_BT
+	current_thread_info()->tango_syscall = 0;
+#endif
+	current->mm->context.flags = is_aarch32_compat_task() ? MMCF_AARCH32 : 0;
 }
 
 #ifdef CONFIG_GCC_PLUGIN_STACKLEAK
diff --git a/arch/arm64/kernel/ptrace.c b/arch/arm64/kernel/ptrace.c
--- a/arch/arm64/kernel/ptrace.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/ptrace.c	2019-04-29 10:13:32.162998400 +0800
@@ -191,7 +191,7 @@
 	info.si_addr	= (void __user *)(bkpt->trigger);
 
 #ifdef CONFIG_COMPAT
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		int si_errno = 0;
 		int i;
 
diff --git a/arch/arm64/kernel/signal.c b/arch/arm64/kernel/signal.c
--- a/arch/arm64/kernel/signal.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/signal.c	2019-04-29 10:13:32.162998400 +0800
@@ -809,7 +809,7 @@
 	/*
 	 * Set up the stack frame
 	 */
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		if (ksig->ka.sa.sa_flags & SA_SIGINFO)
 			ret = compat_setup_rt_frame(usig, ksig, oldset, regs);
 		else
diff --git a/arch/arm64/kernel/syscall.c b/arch/arm64/kernel/syscall.c
--- a/arch/arm64/kernel/syscall.c	2019-04-29 10:13:10.602997832 +0800
+++ b/arch/arm64/kernel/syscall.c	2019-04-29 10:13:32.158998400 +0800
@@ -20,7 +20,7 @@
 {
 #ifdef CONFIG_COMPAT
 	long ret;
-	if (is_compat_task()) {
+	if (is_aarch32_compat_task()) {
 		ret = compat_arm_syscall(regs, scno);
 		if (ret != -ENOSYS)
 			return ret;
@@ -91,6 +91,9 @@
 		local_daif_mask();
 		flags = current_thread_info()->flags;
 		if (!has_syscall_work(flags)) {
+#ifdef CONFIG_TANGO_BT
+			current_thread_info()->tango_syscall = 0;
+#endif
 			/*
 			 * We're off to userspace, where interrupts are
 			 * always enabled after we restore the flags from
@@ -104,6 +107,9 @@
 
 trace_exit:
 	syscall_trace_exit(regs);
+#ifdef CONFIG_TANGO_BT
+	current_thread_info()->tango_syscall = 0;
+#endif
 }
 
 static inline void sve_user_discard(void)
@@ -126,6 +132,13 @@
 asmlinkage void el0_svc_handler(struct pt_regs *regs)
 {
 	sve_user_discard();
+#ifdef CONFIG_TANGO_BT
+	if (regs->regs[8] & 0x80000000) {
+		current_thread_info()->tango_syscall = 1;
+		el0_svc_common(regs, regs->regs[7], __NR_compat_syscalls,
+			       compat_sys_call_table);
+	} else
+#endif
 	el0_svc_common(regs, regs->regs[8], __NR_syscalls, sys_call_table);
 }
 
diff --git a/arch/arm64/mm/mmap.c b/arch/arm64/mm/mmap.c
--- a/arch/arm64/mm/mmap.c	2019-04-29 10:13:10.590997832 +0800
+++ b/arch/arm64/mm/mmap.c	2019-04-29 10:13:32.150998400 +0800
@@ -28,6 +28,8 @@
 #include <linux/io.h>
 #include <linux/personality.h>
 #include <linux/random.h>
+#include <linux/security.h>
+#include <linux/hugetlb.h>
 
 #include <asm/cputype.h>
 
@@ -38,6 +40,13 @@
 #define MIN_GAP (SZ_128M)
 #define MAX_GAP	(STACK_TOP/6*5)
 
+#ifdef CONFIG_TANGO_BT
+/* Definitions for Tango's guest mmap area */
+#define TANGO_STACK_TOP			0xffff0000
+#define TANGO_MAX_GAP			(TANGO_STACK_TOP/6*5)
+#define TANGO_TASK_UNMAPPED_BASE	PAGE_ALIGN(TASK_SIZE_32 / 4)
+#endif
+
 static int mmap_is_legacy(struct rlimit *rlim_stack)
 {
 	if (current->personality & ADDR_COMPAT_LAYOUT)
@@ -79,6 +88,20 @@
 	return PAGE_ALIGN(STACK_TOP - gap - rnd);
 }
 
+#ifdef CONFIG_TANGO_BT
+static unsigned long tango_mmap_base(unsigned long rnd)
+{
+	unsigned long gap = rlimit(RLIMIT_STACK);
+
+	if (gap < MIN_GAP)
+		gap = MIN_GAP;
+	else if (gap > TANGO_MAX_GAP)
+		gap = TANGO_MAX_GAP;
+
+	return PAGE_ALIGN(TANGO_STACK_TOP - gap - rnd);
+}
+#endif
+
 /*
  * This function, called very early during the creation of a new process VM
  * image, sets up which VM layout function to use:
@@ -86,9 +109,18 @@
 void arch_pick_mmap_layout(struct mm_struct *mm, struct rlimit *rlim_stack)
 {
 	unsigned long random_factor = 0UL;
+#ifdef CONFIG_TANGO_BT
+	unsigned long tango_random_factor = 0UL;
+#endif
 
-	if (current->flags & PF_RANDOMIZE)
+	if (current->flags & PF_RANDOMIZE) {
 		random_factor = arch_mmap_rnd();
+#ifdef CONFIG_TANGO_BT
+		tango_random_factor = (get_random_long() &
+			((1UL << mmap_rnd_compat_bits) - 1)) << PAGE_SHIFT;
+#endif
+
+	}
 
 	/*
 	 * Fall back to the standard layout if the personality bit is set, or
@@ -97,9 +129,16 @@
 	if (mmap_is_legacy(rlim_stack)) {
 		mm->mmap_base = TASK_UNMAPPED_BASE + random_factor;
 		mm->get_unmapped_area = arch_get_unmapped_area;
+#ifdef CONFIG_TANGO_BT
+		mm->context.tango_mmap_base = TANGO_TASK_UNMAPPED_BASE +
+			tango_random_factor;
+#endif
 	} else {
 		mm->mmap_base = mmap_base(random_factor, rlim_stack);
 		mm->get_unmapped_area = arch_get_unmapped_area_topdown;
+#ifdef CONFIG_TANGO_BT
+		mm->context.tango_mmap_base = tango_mmap_base(tango_random_factor);
+#endif
 	}
 }
 
@@ -151,3 +190,183 @@
 }
 
 #endif
+
+#ifdef CONFIG_TANGO_BT
+
+/* Get an address range which is currently unmapped.
+ * For shmat() with addr=0.
+ *
+ * Ugly calling convention alert:
+ * Return value with the low bits set means error value,
+ * ie
+ *	if (ret & ~PAGE_MASK)
+ *		error = ret;
+ *
+ * This function "knows" that -ENOMEM has the bits set.
+ */
+unsigned long
+arch_get_unmapped_area(struct file *filp, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma, *prev;
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	if (len > TASK_SIZE - mmap_min_addr)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_tango_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED)
+		return bad_addr ? -ENOMEM : addr;
+
+	if (addr && !bad_addr) {
+		addr = PAGE_ALIGN(addr);
+		vma = find_vma_prev(mm, addr, &prev);
+		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
+		    (!vma || addr + len <= vm_start_gap(vma)) &&
+		    (!prev || addr >= vm_end_gap(prev)))
+			return addr;
+	}
+
+	info.flags = 0;
+	info.length = len;
+	if (is_tango_compat_task()) {
+		info.low_limit = mm->context.tango_mmap_base;
+		info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+	} else {
+		info.low_limit = mm->mmap_base;
+		info.high_limit = TASK_SIZE;
+	}
+	info.align_mask = 0;
+	return vm_unmapped_area(&info);
+}
+
+/*
+ * This mmap-allocator allocates new areas top-down from below the
+ * stack's low limit (the base):
+ */
+unsigned long
+arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,
+			  const unsigned long len, const unsigned long pgoff,
+			  const unsigned long flags)
+{
+	struct vm_area_struct *vma, *prev;
+	struct mm_struct *mm = current->mm;
+	unsigned long addr = addr0;
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	/* requested length too big for entire address space */
+	if (len > TASK_SIZE - mmap_min_addr)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_tango_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED)
+		return bad_addr ? -ENOMEM : addr;
+
+	/* requesting a specific address */
+	if (addr && !bad_addr) {
+		addr = PAGE_ALIGN(addr);
+		vma = find_vma_prev(mm, addr, &prev);
+		if (TASK_SIZE - len >= addr && addr >= mmap_min_addr &&
+				(!vma || addr + len <= vm_start_gap(vma)) &&
+				(!prev || addr >= vm_end_gap(prev)))
+			return addr;
+	}
+
+	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
+	info.length = len;
+	info.low_limit = max(PAGE_SIZE, mmap_min_addr);
+	if (is_tango_compat_task())
+		info.high_limit = mm->context.tango_mmap_base;
+	else
+		info.high_limit = mm->mmap_base;
+	info.align_mask = 0;
+	addr = vm_unmapped_area(&info);
+
+	/*
+	 * A failed mmap() very likely causes application failure,
+	 * so fall back to the bottom-up function here. This scenario
+	 * can happen with large stack limits and large mmap()
+	 * allocations.
+	 */
+	if (offset_in_page(addr)) {
+		VM_BUG_ON(addr != -ENOMEM);
+		info.flags = 0;
+		if (is_tango_compat_task()) {
+			info.low_limit = TANGO_TASK_UNMAPPED_BASE;
+			info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+		} else {
+			info.low_limit = TASK_UNMAPPED_BASE;
+			info.high_limit = TASK_SIZE;
+		}
+		addr = vm_unmapped_area(&info);
+	}
+
+	return addr;
+}
+
+unsigned long
+hugetlb_get_unmapped_area(struct file *file, unsigned long addr,
+		unsigned long len, unsigned long pgoff, unsigned long flags)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	struct hstate *h = hstate_file(file);
+	struct vm_unmapped_area_info info;
+	bool bad_addr = false;
+
+	if (len & ~huge_page_mask(h))
+		return -EINVAL;
+	if (len > TASK_SIZE)
+		return -ENOMEM;
+
+	/*
+	 * Ensure that translated processes do not allocate the last
+	 * page of the 32-bit address space, or anything above it.
+	 */
+	if (is_tango_compat_task())
+		bad_addr = addr + len > TASK_SIZE_32 - PAGE_SIZE;
+
+	if (flags & MAP_FIXED) {
+		if (prepare_hugepage_range(file, addr, len))
+			return -EINVAL;
+		return bad_addr ? -ENOMEM : addr;
+	}
+
+	if (addr && !bad_addr) {
+		addr = ALIGN(addr, huge_page_size(h));
+		vma = find_vma(mm, addr);
+		if (TASK_SIZE - len >= addr &&
+		    (!vma || addr + len <= vm_start_gap(vma)))
+			return addr;
+	}
+
+	info.flags = 0;
+	info.length = len;
+	if (is_tango_compat_task()) {
+		info.low_limit = TANGO_TASK_UNMAPPED_BASE;
+		info.high_limit = TASK_SIZE_32 - PAGE_SIZE;
+	} else {
+		info.low_limit = TASK_UNMAPPED_BASE;
+		info.high_limit = TASK_SIZE;
+	}
+	info.align_mask = PAGE_MASK & ~huge_page_mask(h);
+	info.align_offset = 0;
+	return vm_unmapped_area(&info);
+}
+
+#endif
